<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>spark &#8211; Kshitij Kulshrestha</title>
	<atom:link href="https://kshitij-kuls.com/tag/spark/feed/" rel="self" type="application/rss+xml" />
	<link>https://kshitij-kuls.com</link>
	<description>A DREAM WITHOUT A GOAL IS JUST A DREAM</description>
	<lastBuildDate>Sun, 13 Oct 2019 03:43:16 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>

<image>
	<url>https://kshitijkuls.files.wordpress.com/2018/01/cropped-whatsapp-image-2018-01-04-at-21-23-55.jpeg?w=32</url>
	<title>spark &#8211; Kshitij Kulshrestha</title>
	<link>https://kshitij-kuls.com</link>
	<width>32</width>
	<height>32</height>
</image> 
<cloud domain='kshitij-kuls.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<atom:link rel="search" type="application/opensearchdescription+xml" href="https://kshitij-kuls.com/osd.xml" title="Kshitij Kulshrestha" />
	<atom:link rel='hub' href='https://kshitij-kuls.com/?pushpress=hub'/>
	<item>
		<title>Playing with joins in spark</title>
		<link>https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/</link>
					<comments>https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/#comments</comments>
		
		<dc:creator><![CDATA[Kshitij Kulshrestha]]></dc:creator>
		<pubDate>Thu, 26 Sep 2019 02:13:03 +0000</pubDate>
				<category><![CDATA[Apache Spark]]></category>
		<category><![CDATA[Blogs]]></category>
		<category><![CDATA[joins]]></category>
		<category><![CDATA[optimization]]></category>
		<category><![CDATA[pyspark]]></category>
		<category><![CDATA[spark]]></category>
		<category><![CDATA[Spark SQL]]></category>
		<guid isPermaLink="false">http://kshitij-kuls.com/?p=366</guid>

					<description><![CDATA[It&#8217;s a very common problem across whole data industry, people are struggling to reduce the time complexity, they bought big machines, they moved to spark but yet, there are few problems which can&#8217;t be solved by just using something, it requires a deeper insight to literally feel how the framework works and why it&#8217;s very... <a class="more-link" href="https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/#more-366">Continue Reading &#8594;</a>]]></description>
										<content:encoded><![CDATA[<p style="text-align:justify;">It&#8217;s a very common problem across whole data industry, people are struggling to reduce the time complexity, they bought big machines, they moved to spark but yet, there are few problems which can&#8217;t be solved by just using something, it requires a deeper insight to literally feel how the framework works and why it&#8217;s very slow even with considerable amount of resources.</p>
<p style="text-align:justify;">Here we&#8217;ll be discussing how spark treats a join and figure out how to join.</p>
<p style="text-align:justify;"><span id="more-366"></span></p>
<p style="text-align:justify;">Before we go into much detail, let&#8217;s discuss about varies strategies which spark uses to join.</p>
<p style="text-align:justify;">There are 3 types of strategies:</p>
<ol style="text-align:justify;">
<li><strong>Sort-merge Join: </strong>The keys from both the sides are being sorted and then merged and then data is <strong>shuffled</strong> to bring down the same set of keys on same machine so that joining can be done.<br />
<img data-attachment-id="371" data-permalink="https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/screenshot-2019-09-26-at-10-08-08-am/" data-orig-file="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png" data-orig-size="1046,1378" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2019-09-26 at 10.08.08 AM" data-image-description="" data-medium-file="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=228" data-large-file="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=736" class=" size-full wp-image-371 aligncenter" src="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=736" alt="Screenshot 2019-09-26 at 10.08.08 AM.png" srcset="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=736 736w, https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=114 114w, https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=228 228w, https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=768 768w, https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png?w=777 777w, https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png 1046w" sizes="(max-width: 736px) 100vw, 736px"   /></li>
<li><strong>Broadcast-hash Join: </strong>The smallest side is being broadcasted to each node wherever the largest side of data partition resides, then the joining takes place, this eliminates the shuffling process for bigger dataset.
<p><img data-attachment-id="372" data-permalink="https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/spark_broadcast/" data-orig-file="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png" data-orig-size="528,797" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="spark_broadcast" data-image-description="" data-medium-file="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png?w=199" data-large-file="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png?w=528" class="alignnone size-full wp-image-372" src="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png?w=736" alt="spark_broadcast.png" srcset="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png 528w, https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png?w=99 99w, https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png?w=199 199w" sizes="(max-width: 528px) 100vw, 528px"   /></li>
<li><strong>Shuffle-hash Join:</strong> This is similar to the join in map reduce, it doesn&#8217;t sort the data, it simply hash the keys of both datasets and shuffle the data to the nodes by <strong>(hash mod no. of executors)</strong>.</li>
</ol>
<p style="text-align:justify;">When both side datasets are too big, then it&#8217;s better to use <strong>Sort-merge join</strong> or <strong>Shuffle-hash join</strong> depending on the data locality.</p>
<p style="text-align:justify;">After spark 2.3 release, spark made Sort merge join as the default strategy which can be disabled by setting ‘<strong class="ko ma">spark.sql.join.preferSortMergeJoin</strong>’ = <strong>false</strong>.</p>
<p style="text-align:justify;"><strong>Broadcast-hash Join</strong></p>
<p>Best suitable when one of the dataset is small [about 1 GB]. There are 2 way of achieving this:</p>
<ol>
<li>By setting <strong class="ko ma">spark.sql.autoBroadcastJoinThreshold</strong>, this parameter takes value in bytes and default is 10 MB, spark check the size of dataset and it found it to be lesser than the threshold value then it will broadcast this dataset automatically to all the nodes wherever big dataset partitions reside.
<p><strong>**</strong> Sometimes it may happen that spark may not be able to figure out the size of data correctly, which may end up in <strong>sort-merge join</strong>, so to get rid if this, one can <strong>persist</strong> the dataset on <strong>disk and memory,</strong> by doing this spark will know the actual size of data and will choose the strategy accordingly.</li>
<li><strong>Forcing spark to broadcast</strong>, spark has also provided a functionality to broadcast a dataset forcefully, this approach is advisable only when we are sure about the data sizing.</li>
</ol>
<p>&nbsp;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://kshitij-kuls.com/2019/09/26/playing-with-joins-in-spark/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/3eb7eaa9690ab587122cc92adb65d0b7?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">kshitijk23</media:title>
		</media:content>

		<media:content url="https://kshitijkuls.files.wordpress.com/2019/09/screenshot-2019-09-26-at-10.08.08-am.png" medium="image">
			<media:title type="html">Screenshot 2019-09-26 at 10.08.08 AM.png</media:title>
		</media:content>

		<media:content url="https://kshitijkuls.files.wordpress.com/2019/09/spark_broadcast.png" medium="image">
			<media:title type="html">spark_broadcast.png</media:title>
		</media:content>
	</item>
		<item>
		<title>Understanding the flow of classpath to run spark job on yarn</title>
		<link>https://kshitij-kuls.com/2019/08/04/understanding-the-flow-of-classpath-to-run-spark-job-on-yarn/</link>
					<comments>https://kshitij-kuls.com/2019/08/04/understanding-the-flow-of-classpath-to-run-spark-job-on-yarn/#respond</comments>
		
		<dc:creator><![CDATA[Kshitij Kulshrestha]]></dc:creator>
		<pubDate>Sun, 04 Aug 2019 04:09:14 +0000</pubDate>
				<category><![CDATA[Apache Spark]]></category>
		<category><![CDATA[Blogs]]></category>
		<category><![CDATA[Spark on Yarn]]></category>
		<category><![CDATA[classpath]]></category>
		<category><![CDATA[spark]]></category>
		<category><![CDATA[yarn]]></category>
		<guid isPermaLink="false">http://kshitij-kuls.com/?p=334</guid>

					<description><![CDATA[To run a job on cluster, it&#8217;s very necessary to provide correct set of jars but it&#8217;s always challenging on clustered environment where we have to deal with lots of moving components with different set of requirements and hence it&#8217;s very important to understand the importance of classpath settings otherwise a one can land up... <a class="more-link" href="https://kshitij-kuls.com/2019/08/04/understanding-the-flow-of-classpath-to-run-spark-job-on-yarn/#more-334">Continue Reading &#8594;</a>]]></description>
										<content:encoded><![CDATA[<p class="auto-cursor-target" style="text-align:justify;">To run a job on cluster, it&#8217;s very necessary to provide correct set of jars but it&#8217;s always challenging on clustered environment where we have to deal with lots of moving components with different set of requirements and hence it&#8217;s very important to understand the importance of classpath settings otherwise a one can land up to a never ending problem <strong>ClassNotFoundException</strong>.</p>
<p class="auto-cursor-target" style="text-align:justify;"><strong><span id="more-334"></span><br />
&lt;CPS&gt; </strong>Yarn would resolve this key work with <strong>&#8216;:&#8217;</strong></p>
<p class="auto-cursor-target" style="text-align:justify;"><strong>&lt;PWD&gt; </strong>Yarn would resolve this key with the working directory.</p>
<p style="text-align:justify;"><strong>YARN-ENV-ENTRIES</strong></p>
<p style="text-align:justify;"><code>CLASSPATH</code>: Jars specified in this text-box would be used loaded on Yarn-Launcher class path and will be used to launch a Spark Job. This place is a way to isolate the dependencies of spark from yarn,</p>
<p style="text-align:justify;">                       Example:</p>
<div class="code panel pdl conf-macro output-block" style="text-align:justify;">
<div class="codeHeader panelHeader pdl"><b>CLASSPATH</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_617751" class="syntaxhighlighter sh-confluence nogutter  bash">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hive/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hive/lib/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-yarn/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/hadoop/lib/</code><code class="bash plain">*&lt;CPS&gt;</code><code class="bash plain">/opt/cloudera/parcels/CDH/lib/spark/jars/</code><code class="bash plain">*</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p class="auto-cursor-target" style="text-align:justify;">We usually need hadoop jars, scala jars and spark jars to submit a spark job.</p>
<p class="auto-cursor-target" style="text-align:justify;"><strong>YARN-HADOOP</strong></p>
<div class="col-md-3" style="text-align:justify;"><code>yarn.mode.hadoop.classpath</code>: This will point to the hadoop jars but it&#8217;s not required to set this if we have already set the above property.</div>
<div class="col-md-3" style="text-align:justify;"><strong><br />
SPARK</strong></div>
<div class="col-md-3" style="text-align:justify;">
<div></div>
<div class="col-md-3"><code>spark.driver.extraClassPath</code>: We can use this property to provide jars to driver node, the hadoop jars and all the other jars which is already localised(already present on all the nodes). These jars would also be available from the start of the spark job.</div>
<div></div>
</div>
<div class="col-md-3" style="text-align:justify;">
<hr />
</div>
<div class="col-md-3" style="text-align:justify;">
<div></div>
<div class="col-md-3"><code>spark.executor.extraClassPath</code>: We can use this property to provide jars to all executor nodes, the hadoop jars and all the other jars  which is already localised(already present on all the nodes).</div>
<div></div>
</div>
<div class="col-md-3" style="text-align:justify;">
<hr />
</div>
<div style="text-align:justify;"></div>
<div class="col-md-3" style="text-align:justify;"><code>spark.yarn.archive</code>: We can provide a compressed file containing jars from hdfs itself to be present on spark job class path and which is not available locally, These jars would not be available till spark context</div>
<div class="col-md-3" style="text-align:justify;">get created<strong> so putting hadoop jars or any other jar in yarn-archive will not work in the launch of spark job and it will fail with class not found exception.</strong></div>
<div style="text-align:justify;"></div>
<div class="col-md-3" style="text-align:justify;">
<div class="col-md-3">
<hr />
</div>
<div></div>
<div class="col-md-3"><code>spark.yarn.stagingDir</code>: This is used by spark to put all the jars and other files to be localised, make sure if driver fails with jar not found then please do either of this:</div>
<div></div>
<div><span style="color:#ffffff;">&#8212;&#8212;&#8212;</span></div>
<div>1. Set this property with some hdfs path because it may possible it would be writing to a local node because of some default file system issue.<br />
Example: hdfs://ip-11-0-0-189.ec2.internal:8020/user/</div>
<div></div>
<div><span style="color:#ffffff;">&#8212;&#8212;&#8212;</span><br />
2. Put correct set configuration files (core-site.xml, hdfs-site.xml and other hadoop configuration files) on the classpath. If we don&#8217;t put these files on classpath while submitting the spark then spark would not know that it&#8217;s the same cluster where you are running the spark job and hence it will take long time to submit spark job.</div>
<div></div>
</div>
<div class="col-md-3" style="text-align:justify;">
<hr />
</div>
<div style="text-align:justify;"></div>
<div class="col-md-3" style="text-align:justify;"><code>spark.files</code>: Any external file can be passed to spark job using this property, in our case we want hive-site.xml on the classpath to connect to metastore. Example: <code>/etc/hive/conf/hive-site.xml</code></div>
]]></content:encoded>
					
					<wfw:commentRss>https://kshitij-kuls.com/2019/08/04/understanding-the-flow-of-classpath-to-run-spark-job-on-yarn/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/3eb7eaa9690ab587122cc92adb65d0b7?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">kshitijk23</media:title>
		</media:content>
	</item>
		<item>
		<title>Setting up Virtual Environment for Pyspark or any other clustered env</title>
		<link>https://kshitij-kuls.com/2019/08/04/setting-up-virtual-environment-for-pyspark/</link>
					<comments>https://kshitij-kuls.com/2019/08/04/setting-up-virtual-environment-for-pyspark/#respond</comments>
		
		<dc:creator><![CDATA[Kshitij Kulshrestha]]></dc:creator>
		<pubDate>Sun, 04 Aug 2019 02:59:49 +0000</pubDate>
				<category><![CDATA[python]]></category>
		<category><![CDATA[Virtual Environment]]></category>
		<category><![CDATA[dynload]]></category>
		<category><![CDATA[pyspark]]></category>
		<category><![CDATA[spark]]></category>
		<guid isPermaLink="false">http://kshitij-kuls.com/?p=307</guid>

					<description><![CDATA[On clustered environment, we face lot of issues with the python version available on the nodes, if we are shipping our product in that case we had to perform lot of sanity test pre-deployment to make sure our application will run as per our expectation but we can&#8217;t cover all scenarios and hence there is... <a class="more-link" href="https://kshitij-kuls.com/2019/08/04/setting-up-virtual-environment-for-pyspark/#more-307">Continue Reading &#8594;</a>]]></description>
										<content:encoded><![CDATA[<div>
<div class="sc-dTdPqK DYtDj">
<div id="content" class="page view">
<div id="main-content" class="wiki-content">
<p>On clustered environment, we face lot of issues with the python version available on the nodes, if we are shipping our product in that case we had to perform lot of sanity test pre-deployment to make sure our application will run as per our expectation but we can&#8217;t cover all scenarios and hence there is high chance of hitting issue.</p>
</div>
<p>So we thought of a better way and come up with an idea of shipping our own python version with everything preinstalled in that package, everyone might have been familiar with <strong>Virtual Environment or Anaconda </strong>but believe me after reading this you would get something new to learn.</p>
<div id="main-content" class="wiki-content">
<p><span id="more-307"></span>Before we proceed it&#8217;s require to understand the basic structure of python:</p>
<p>├── bin<br />
│ ├── activate<br />
│ ├── activate.csh<br />
│ ├── activate.fish<br />
│ ├── activate_this.py<br />
│ ├── easy_install<br />
│ ├── easy_install-3.6<br />
│ ├── pip<br />
│ ├── pip3<br />
│ ├── pip3.6<br />
│ ├── python<br />
│ ├── python-config<br />
│ ├── python3 -&gt; python<br />
│ ├── python3.6 -&gt; python<br />
│ └── wheel<br />
├── include<br />
│ └── python3.6m -&gt; /usr/include/python3.6m<br />
├── lib<br />
│ └── python3.6<br />
| ├── site-packages<br />
│ ├── lib-dynload -&gt; /usr/lib/python3.6/lib-dynload [Dynamic Library]</p>
<p><em><u><strong>Environment Variables:</strong></u></em></p>
<p><strong>PYSPARK_PYTHON : Points to the executable python file: bin/python</strong></p>
<p><strong>LD_LIBRARY_PATH : Points to the dynamic library path: lib/python3.6/lib-dynload [All .so* files]</strong></p>
<p><strong>PYTHONPATH : Points to the installed packages within virtual environment as well as the dynamic library path : lib/python3.6/site-packages&lt;CPS&gt;lib/python3.6/lib-dynload [All .py files]</strong></p>
<p><strong>PYTHONHOME : Points to the python library path: lib/python3.6/site-packages</strong></p>
<p>Steps to build Virtual environment:</p>
<ol>
<li>Install python in the machine of desired version.</li>
<li>
<div class="code panel pdl conf-macro output-block">
<div class="codeHeader panelHeader pdl"><b>Create Virtual Env</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_262334" class="syntaxhighlighter sh-confluence nogutter  py">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="py plain">virtualenv env </code><code class="py keyword">-</code><code class="py plain">p </code><code class="py keyword">/</code><code class="py plain">usr</code><code class="py keyword">/</code><code class="py plain">local</code><code class="py keyword">/</code><code class="py functions">bin</code><code class="py keyword">/</code><code class="py plain">python3</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</li>
<li>
<div class="code panel pdl conf-macro output-block">
<div class="codeHeader panelHeader pdl"><b>Activate Virtual Env</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_195203" class="syntaxhighlighter sh-confluence nogutter  py">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="py plain">source env</code><code class="py keyword">/</code><code class="py functions">bin</code><code class="py keyword">/</code><code class="py plain">activate</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</li>
<li>
<div class="code panel pdl conf-macro output-block">
<div class="codeHeader panelHeader pdl"><b>Install requirements</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_304113" class="syntaxhighlighter sh-confluence nogutter  py">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="py plain">pip install numpy</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</li>
<li>Now here is the trick, you can see this line ├── lib-dynload -&gt; /usr/lib/python3.6/lib-dynload it&#8217;s a symbolic link and pointing to the local machine path and hence even if you just zip this virtual environment folder then these dependencies would be missing on the cluster.</li>
<li>So, it&#8217;s required to copy all the <em><u><strong>.so*</strong></u></em> files from /usr/lib/python3.6/lib-dynload, /usr/lib64/*.so.*, etc&#8230; to <strong>lib/python3.6/lib-dynload </strong> [Be careful about  /usr/lib64/*.so.*, it does contain os specific libs, which may fail on different os versions, hence try to avoid so files from this specific folder].</li>
<li>Copy all the <em><u><strong>.py</strong></u></em>files from /usr/lib/python3.6/lib-dynload, /usr/lib64/*.so.*, etc&#8230; to <strong>lib/python3.6/site-packages.</strong></li>
<li>
<p class="auto-cursor-target">Run it from the home dir of virtual environment in our case it&#8217;s <strong>env/</strong></p>
<div class="code panel pdl conf-macro output-block">
<div class="codeHeader panelHeader pdl"><b>Prepare zip</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_332454" class="syntaxhighlighter sh-confluence nogutter  bash">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="bash plain">zip -rq ..</code><code class="bash plain">/venv</code><code class="bash plain">.zip *</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</li>
</ol>
<p><u><em><strong>Environmental variable setup</strong></em></u></p>
<p>For <strong>driver</strong>: spark.yarn.appMasterEnv.[Environment variable]</p>
<p>For <strong>executor</strong>: spark.executorEnv.[Environment variable]</p>
<p><strong>PYSPARK_PYTHON</strong></p>
<ol>
<li class="col-md-3">pyspark.spark.yarn.appMasterEnv.PYSPARK_PYTHON = venv/bin/python</li>
<li class="col-md-3">
<div class="col-md-3">pyspark.spark.executorEnv.PYSPARK_PYTHON = venv/bin/python</div>
</li>
</ol>
<p><strong>PYTHONHOME</strong></p>
<ol>
<li>
<div class="col-md-3">pyspark.spark.yarn.appMasterEnv.PYTHONHOME = venv/lib64/python3.6/site-packages</div>
</li>
<li>
<div class="col-md-3">pyspark.spark.executorEnv.PYTHONHOME = venv/lib64/python3.6/site-packages</div>
</li>
</ol>
<p><strong>LD_LIBRARY_PATH</strong></p>
<ol>
<li>
<div class="col-md-3">pyspark.spark.yarn.appMasterEnv.LD_LIBRARY_PATH = venv/lib64/python3.6/lib-dynload</div>
</li>
<li>
<div class="col-md-3">pyspark.spark.executorEnv.LD_LIBRARY_PATH = venv/lib64/python3.6/lib-dynload</div>
</li>
</ol>
<p><strong>PYTHONPATH</strong></p>
<p>This need to included in <strong>YARN-ENV-ENTRIES</strong>, it&#8217;s not getting set from the spark configs.</p>
<p>PYTHONPATH = {{PWD}}/__venv__.zip&lt;CPS&gt;{{PWD}}/__py4j-0.10.7-src__.zip&lt;CPS&gt;venv/lib64/python3.6/site-packages&lt;CPS&gt;venv/lib64/python3.6/lib-dynload&lt;CPS&gt;</p>
<div class="code panel pdl conf-macro output-block">
<div class="codeHeader panelHeader pdl"><b>To run python</b></div>
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_796347" class="syntaxhighlighter sh-confluence nogutter  bash">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="bash functions">cd</code> <code class="bash plain">venv</code></div>
<div class="line number2 index1 alt1"></div>
<div class="line number3 index2 alt2"><code class="bash functions">export</code> <code class="bash plain">PYTHONPATH=lib64</code><code class="bash plain">/python3</code><code class="bash plain">.6</code><code class="bash plain">/site-packages</code><code class="bash plain">:lib64</code><code class="bash plain">/python3</code><code class="bash plain">.6</code><code class="bash plain">/lib-dynload/</code></div>
<div class="line number4 index3 alt1"></div>
<div class="line number5 index4 alt2"><code class="bash functions">export</code> <code class="bash plain">LD_LIBRARY_PATH=lib64</code><code class="bash plain">/python3</code><code class="bash plain">.6</code><code class="bash plain">/lib-dynload</code></div>
<div class="line number6 index5 alt1"></div>
<div class="line number7 index6 alt2"><code class="bash functions">source</code> <code class="bash plain">bin</code><code class="bash plain">/activate</code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="likes-and-labels-container" class="ViewPage_likesAndLabelsContainer_nyS">
<div></div>
</div>
]]></content:encoded>
					
					<wfw:commentRss>https://kshitij-kuls.com/2019/08/04/setting-up-virtual-environment-for-pyspark/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/3eb7eaa9690ab587122cc92adb65d0b7?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">kshitijk23</media:title>
		</media:content>
	</item>
	</channel>
</rss>
